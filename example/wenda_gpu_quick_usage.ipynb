{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: basic wenda_gpu usage\n",
    "\n",
    "This notebook walks through the basic steps needed to run wenda_gpu on a small, simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537e0f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wenda_gpu import wenda_gpu as wg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d04330",
   "metadata": {},
   "source": [
    "The variable \"prefix\" is intended to be a unique identifier for your dataset, which allows you to run wenda_gpu on multiple datasets and have them nested within the same directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5aabfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"simulated_example\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f97198",
   "metadata": {},
   "source": [
    "The source and target datasets should each be a matrix where each column is a feature and each row is a sample. Thus, the source and target data files must have the same number of columns, but need not have the same number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d60350d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6657  0.1712  0.6055 ... -0.3662 -0.079  -0.9831]\n",
      " [-0.5039 -0.4653 -0.6866 ...  0.649  -0.2828  0.5439]\n",
      " [-0.6244 -0.69    0.0616 ...  1.3983 -0.2632  1.7183]\n",
      " ...\n",
      " [-1.184  -2.1196 -1.191  ...  2.3368 -0.7823  0.5206]\n",
      " [-0.1836  0.6838  0.2268 ... -1.4623  0.2442 -0.2596]\n",
      " [-1.7111 -1.7859 -1.6365 ... -0.401   1.1332 -1.9584]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1200, 500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_x, target_x = wg.load_data(prefix=prefix, data_path=\"data\")\n",
    "\n",
    "print(source_x)\n",
    "source_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7aecf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1909 -1.0174 -0.7121 ...  0.9328 -3.1927  0.2734]\n",
      " [ 0.9062  1.445   0.8685 ... -0.1718 -1.825  -0.7571]\n",
      " [-1.8884 -1.4984 -1.0887 ... -1.3003  1.2522 -1.1692]\n",
      " ...\n",
      " [-0.7921 -0.8746 -0.5454 ... -0.3572  2.646   1.5304]\n",
      " [-0.1447 -0.4359  0.0337 ...  1.3563 -0.3737 -0.9744]\n",
      " [ 1.1162  1.7021  1.4116 ... -0.4561  2.0629  0.1082]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(target_x)\n",
    "target_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb701bf",
   "metadata": {},
   "source": [
    "If your data is already stored somewhere without the prefix directory structure or with different file naming conventions, you can simply load the files manually and then convert them to a numpy fortran array, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cee99b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6657  0.1712  0.6055 ... -0.3662 -0.079  -0.9831]\n",
      " [-0.5039 -0.4653 -0.6866 ...  0.649  -0.2828  0.5439]\n",
      " [-0.6244 -0.69    0.0616 ...  1.3983 -0.2632  1.7183]\n",
      " ...\n",
      " [-1.184  -2.1196 -1.191  ...  2.3368 -0.7823  0.5206]\n",
      " [-0.1836  0.6838  0.2268 ... -1.4623  0.2442 -0.2596]\n",
      " [-1.7111 -1.7859 -1.6365 ... -0.401   1.1332 -1.9584]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1200, 500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_file = \"data/simulated_example/source_data.tsv\"\n",
    "source_table = pd.read_csv(source_file, sep=\"\\t\", header=None)\n",
    "source_x = np.asfortranarray(source_table.values)\n",
    "\n",
    "target_file = \"data/simulated_example/target_data.tsv\"\n",
    "target_table = pd.read_csv(target_file, sep=\"\\t\", header=None)\n",
    "target_x = np.asfortranarray(target_table.values)\n",
    "\n",
    "print(source_x)\n",
    "source_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940235b2",
   "metadata": {},
   "source": [
    "Now that the data is loaded, we will need to normalize both source and target datasets. Both datasets are normalized based on the source data's distribution to allow direct comparison between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c793ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69694417,  0.21877732,  0.66571408, ..., -0.38984496,\n",
       "        -0.0504219 , -1.0386572 ],\n",
       "       [-0.45969857, -0.42033402, -0.64428229, ...,  0.6596134 ,\n",
       "        -0.25281859,  0.52678521],\n",
       "       [-0.57886363, -0.64595589,  0.11428073, ...,  1.43419886,\n",
       "        -0.23335355,  1.73075087],\n",
       "       ...,\n",
       "       [-1.13226418, -2.08142104, -1.15566854, ...,  2.40436894,\n",
       "        -0.74887918,  0.50289863],\n",
       "       [-0.14294697,  0.73348034,  0.28176885, ..., -1.52293332,\n",
       "         0.27055263, -0.296943  ],\n",
       "       [-1.65352477, -1.74635198, -1.60733898, ..., -0.4258193 ,\n",
       "         1.15343123, -2.03851049]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_x_norm, target_x_norm = wg.normalize_data(source_x, target_x)\n",
    "\n",
    "source_x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f0e98f",
   "metadata": {},
   "source": [
    "With the data loaded and normalized, we are ready to train our feature models. Note that this may take several minutes on the training dataset and up to several hours on larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0670e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models 0 to 99...\n",
      "Training models 100 to 199...\n",
      "Training models 200 to 299...\n",
      "Training models 300 to 399...\n",
      "Training models 400 to 499...\n"
     ]
    }
   ],
   "source": [
    "wg.train_feature_models(source_x_norm, target_x_norm, prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a18afc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_466.pth', 'model_95.pth', 'model_99.pth', 'model_176.pth']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_model_files = os.listdir(os.path.join(\"feature_models\",prefix))\n",
    "print(feature_model_files[1:5])\n",
    "len(feature_model_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8545eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_92_confidence.txt', 'model_378_confidence.txt', 'model_488_confidence.txt', 'model_240_confidence.txt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_files = os.listdir(os.path.join(\"confidences\",prefix))\n",
    "print(confidence_files[1:5])\n",
    "len(confidence_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0ab0f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_y = wg.load_labels(prefix)\n",
    "\n",
    "source_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365c395",
   "metadata": {},
   "source": [
    "Again, if your data is stored separately, you can load labels manually and then convert to a numpy fortranarray, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65ffc06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file = \"data/simulated_example/source_y.tsv\"\n",
    "label_table = pd.read_csv(label_file, header=None)\n",
    "source_y = np.asfortranarray(label_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03142998",
   "metadata": {},
   "source": [
    "We now have all the components necessary for running the weighted elastic net. Note that since our labels are binary, we need to set logistic=True to run logistic net regression as opposed to elastic net regression for continuous data. This may take a minute for the example data and several minutes for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58336ed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_wnet = 0\n",
      "k_wnet = 1\n",
      "k_wnet = 2\n",
      "k_wnet = 3\n",
      "k_wnet = 4\n",
      "k_wnet = 6\n",
      "k_wnet = 8\n",
      "k_wnet = 10\n",
      "k_wnet = 14\n",
      "k_wnet = 18\n",
      "k_wnet = 25\n",
      "k_wnet = 35\n"
     ]
    }
   ],
   "source": [
    "wg.train_elastic_net(source_x_norm, source_y, target_x_norm, prefix=prefix, logistic=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20614e8",
   "metadata": {},
   "source": [
    "The predictions of the elastic net model for the target data will be automatically written to the output folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79836bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0\n",
      "0    1\n",
      "1    1\n",
      "2    0\n",
      "3    0\n",
      "4    1\n",
      "..  ..\n",
      "995  1\n",
      "996  1\n",
      "997  0\n",
      "998  0\n",
      "999  1\n",
      "\n",
      "[1000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "results_file = os.path.join(\"output\",prefix,\"k_01/target_predictions.txt\")\n",
    "results = pd.read_csv(results_file, header=None)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9157e91",
   "metadata": {},
   "source": [
    "And since we ran logistic regression, we can also see the assignment probability for each sample.\n",
    "The first column is probability of label 0, second column is probability of label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d107477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           0\n",
      "0    1.45078e-01 8.54922e-01\n",
      "1    3.50972e-02 9.64903e-01\n",
      "2    9.99732e-01 2.68061e-04\n",
      "3    9.73876e-01 2.61243e-02\n",
      "4    4.33515e-01 5.66485e-01\n",
      "..                       ...\n",
      "995  2.86878e-01 7.13122e-01\n",
      "996  1.27318e-01 8.72682e-01\n",
      "997  9.99943e-01 5.74289e-05\n",
      "998  5.46992e-01 4.53008e-01\n",
      "999  6.63956e-04 9.99336e-01\n",
      "\n",
      "[1000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "probability_file = os.path.join(\"output\",prefix,\"k_01/target_probabilities.txt\")\n",
    "probabilities = pd.read_csv(probability_file,header=None)\n",
    "\n",
    "print(probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
