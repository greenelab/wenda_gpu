{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps]: Warning, no cuda detected. Switching to cpu only.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.float64 to run in double precision, torch.float32 for single\n",
    "dtype = torch.float64\n",
    "\n",
    "# 'cuda' for GPU, 'cpu' for CPU\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('gpy_data.pth'):\n",
    "    raise RuntimeError(\"Run gpy_wenda_save_data.ipynb first!\")\n",
    "    \n",
    "train_x, train_y, test_x, test_y = torch.load('gpy_data.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1866, 12979]) torch.Size([1866]) torch.Size([1001, 12979]) torch.Size([1001])\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x.to(device=device, dtype=dtype)\n",
    "train_y = train_y.to(device=device, dtype=dtype)\n",
    "test_x = test_x.to(device=device, dtype=dtype)\n",
    "test_y = test_y.to(device=device, dtype=dtype)\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Initialize model and likelihood\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.LinearKernel()\n",
    "  \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "    noise_constraint=gpytorch.constraints.Positive(),\n",
    ").to(device=device, dtype=dtype)\n",
    "\n",
    "model = ExactGPModel(train_x, train_y, likelihood).to(device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from LBFGS import FullBatchLBFGS\n",
    "\n",
    "def train_model_bfgs(model, likelihood, x, y, learning_rate,\n",
    "                training_iter=20):\n",
    "    lbfgs = FullBatchLBFGS(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    def closure():\n",
    "        model.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = -mll(output, y)\n",
    "        return loss\n",
    "\n",
    "    loss = closure()\n",
    "    loss.backward()\n",
    "\n",
    "    train_iter = tqdm(range(training_iter))\n",
    "    for i in train_iter:\n",
    "        options = {\"closure\": closure, \"current_loss\": loss, \"max_ls\": 10}\n",
    "        loss, _, lr, _, F_eval, G_eval, _, fail = lbfgs.step(options)\n",
    "        train_iter.set_postfix({'loss': loss.item(), 'fail': fail})\n",
    "        \n",
    "        if fail:\n",
    "            print('Convergence reached!')\n",
    "            break\n",
    "    \n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f05e219c9264387b80d97cd6d5b5f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\\\wsl.localhost\\Ubuntu\\home\\jacobrg\\wenda_gpu_MRE\\LBFGS\\LBFGS.py:296: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1025.)\n",
      "  p.data.add_(step_size, update[offset : offset + numel].view_as(p.data))\n",
      "C:\\Users\\Gardn\\anaconda3\\lib\\site-packages\\gpytorch\\utils\\cholesky.py:44: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal\n",
      "  warnings.warn(f\"A not p.d., added jitter of {jitter_new:.1e} to the diagonal\", NumericalWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianLikelihood(\n",
       "  (noise_covar): HomoskedasticNoise(\n",
       "    (raw_noise_constraint): Positive()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "with gpytorch.settings.max_cholesky_size(100000):\n",
    "    model, likelihood = train_model_bfgs(\n",
    "        model, likelihood, train_x, train_y, learning_rate=1., training_iter=30\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood noise tensor([3.5572e-14], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftplusBackward>)\n",
      "linear kernel variance tensor([[4.5330e-05]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftplusBackward>)\n"
     ]
    }
   ],
   "source": [
    "# GPy learned:\n",
    "## noise = 3.277223830593155e-15\n",
    "## linear variance = 4.5426621979927965e-05\n",
    "\n",
    "print('likelihood noise', likelihood.noise)\n",
    "print('linear kernel variance', model.covar_module.variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save state dict\n",
    "\n",
    "gene_number = 2\n",
    "feature_model_format = 'model_{0:05d}'\n",
    "output_dir = os.path.join(\"gpytorch_feature_models\", feature_model_format.format(gene_number))\n",
    "if os.path.exists(output_dir) is False:\n",
    "    os.mkdir(output_dir)\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"state_dict.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confidence score based on CDF of true target value on GP model\n",
    "def getConfidence(model, x, y):\n",
    "    with gpytorch.settings.fast_pred_var(), gpytorch.settings.max_cholesky_size(10000):\n",
    "        f_preds = likelihood(model(x))\n",
    "    mu = f_preds.mean\n",
    "    sigma_sq = f_preds.variance\n",
    "    sigma_sq = torch.sqrt(sigma_sq)\n",
    "    res_normed = (y - mu) / sigma_sq\n",
    "    res_normed = res_normed.cpu().detach().numpy()\n",
    "    confidences = (1 - abs(norm.cdf(res_normed) - norm.cdf(-res_normed)))\n",
    "    mu = mu.cpu().detach().numpy()\n",
    "    sigma_sq = sigma_sq.cpu().detach().numpy()\n",
    "    return mu, sigma_sq ** 2, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5862101177467771\n"
     ]
    }
   ],
   "source": [
    "# Write out confidence scores and predicted means and variances on target data\n",
    "mean, var, conf = getConfidence(model, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_file = os.path.join(output_dir, \"confidences.txt\")\n",
    "np.savetxt(conf_file, conf, fmt='%.10f')\n",
    "mean_file = os.path.join(output_dir, \"predicted_means.txt\")\n",
    "np.savetxt(mean_file, mean, fmt='%.5f')\n",
    "var_file = os.path.join(output_dir, \"predicted_variances.txt\")\n",
    "np.savetxt(var_file, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** GPyTorch (fast)\n",
      "NLL      -1.006\n",
      "RMSE      0.394\n",
      "Avg conf. 0.586\n"
     ]
    }
   ],
   "source": [
    "print('** GPyTorch (fast)')\n",
    "print(f'NLL      {torch.distributions.Normal(torch.from_numpy(mean), torch.from_numpy(var).sqrt()).log_prob(test_y.cpu()).mean().item():.3f}')\n",
    "print(f'RMSE      {(torch.from_numpy(mean) - test_y.cpu()).pow(2).mean().item():.3f}')\n",
    "print(f'Avg conf. {conf.mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
